{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891c32ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BARRY'~1\\AppData\\Local\\Temp/ipykernel_14880/2038170223.py:15: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.11.0\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np \n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from PIL import Image\n",
    "import collections\n",
    "from types import SimpleNamespace\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## Certificate error fix\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "## Remove Warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Using torch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9d8a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x257006d5ab0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42) # Setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ffede4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the GPU available? True\n",
      "Device: cuda\n",
      "Device name: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "#Check for cuda GPU to run on:\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "print(f\"Is the GPU available? {gpu_avail}\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "if gpu_avail:\n",
    "    print(\"Device name: \" + torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a2f2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34a4ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Data mean [0.49139968 0.48215841 0.44653091]\n",
      "Data std [0.24703223 0.24348513 0.26158784]\n"
     ]
    }
   ],
   "source": [
    "#Load dataset, calculate mean and std.dev\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, download=True)\n",
    "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))\n",
    "DATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))\n",
    "print(\"Data mean\", DATA_MEANS)\n",
    "print(\"Data std\", DATA_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14a72d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((32,32),scale=(0.8,1.0),ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "set_seed(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "set_seed(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67e528a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION 1, PART A\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9229cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and validation functions for first model:\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "            \n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # <1>\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
    "                total += labels.shape[0]  # <3>\n",
    "                correct += int((predicted == labels).sum())  # <4>\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da6b5704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18354 [432, 16, 1152, 8, 16384, 32, 320, 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f47f6aa2a1476288e8c422a6b9df4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 20:08:02.492844 Epoch 1, Training loss 2.273495835796041\n",
      "2022-03-27 20:09:21.553304 Epoch 10, Training loss 1.4327392109438903\n",
      "2022-03-27 20:10:57.437005 Epoch 20, Training loss 1.2443940481569014\n",
      "2022-03-27 20:12:25.269210 Epoch 30, Training loss 1.1418811086236242\n",
      "2022-03-27 20:14:08.126310 Epoch 40, Training loss 1.0795786954738475\n",
      "2022-03-27 20:15:36.371722 Epoch 50, Training loss 1.0375436800837177\n",
      "2022-03-27 20:17:04.616070 Epoch 60, Training loss 1.008249326818689\n",
      "2022-03-27 20:18:32.505548 Epoch 70, Training loss 0.9804658738636224\n",
      "2022-03-27 20:20:00.270168 Epoch 80, Training loss 0.9613154876945366\n",
      "2022-03-27 20:21:28.311250 Epoch 90, Training loss 0.9421492663883416\n",
      "2022-03-27 20:22:57.390036 Epoch 100, Training loss 0.9301863941032322\n",
      "2022-03-27 20:24:26.988756 Epoch 110, Training loss 0.9155107347713916\n",
      "2022-03-27 20:25:56.958539 Epoch 120, Training loss 0.9059064498654118\n",
      "2022-03-27 20:27:26.833892 Epoch 130, Training loss 0.8981608733152732\n",
      "2022-03-27 20:28:59.193207 Epoch 140, Training loss 0.8869243878924269\n",
      "2022-03-27 20:30:30.325480 Epoch 150, Training loss 0.8801297193918473\n",
      "2022-03-27 20:32:01.595113 Epoch 160, Training loss 0.8659574311343353\n",
      "2022-03-27 20:33:32.856109 Epoch 170, Training loss 0.861108087236725\n",
      "2022-03-27 20:35:04.511564 Epoch 180, Training loss 0.8532054244283258\n",
      "2022-03-27 20:36:36.298959 Epoch 190, Training loss 0.846900816996213\n",
      "2022-03-27 20:38:07.710673 Epoch 200, Training loss 0.8430947167241675\n",
      "2022-03-27 20:39:38.438828 Epoch 210, Training loss 0.8364296937600161\n",
      "2022-03-27 20:41:10.314322 Epoch 220, Training loss 0.828115157591991\n",
      "2022-03-27 20:42:42.734451 Epoch 230, Training loss 0.8243778076266971\n",
      "2022-03-27 20:44:14.473913 Epoch 240, Training loss 0.820863536789886\n",
      "2022-03-27 20:45:45.882454 Epoch 250, Training loss 0.8137361794455439\n",
      "2022-03-27 20:47:17.471803 Epoch 260, Training loss 0.816863776951434\n",
      "2022-03-27 20:48:49.063997 Epoch 270, Training loss 0.8135675577356605\n",
      "2022-03-27 20:50:20.205431 Epoch 280, Training loss 0.8051606574629107\n",
      "2022-03-27 20:51:51.815042 Epoch 290, Training loss 0.800841269160268\n",
      "2022-03-27 20:53:22.752032 Epoch 300, Training loss 0.8000113968326156\n",
      "Accuracy train: 0.73\n",
      "Accuracy val: 0.70\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = Net().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "training_loop(  \n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b938c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION 1, PART B\n",
    "class NetB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 4, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * 4)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edc70b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4310 [432, 16, 1152, 8, 288, 4, 2048, 32, 320, 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fead3015a4824851aee1caddeaa98bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 20:53:48.006275 Epoch 1, Training loss 2.3039934051002873\n",
      "2022-03-27 20:55:10.535255 Epoch 10, Training loss 1.8366410586229416\n",
      "2022-03-27 20:56:41.824127 Epoch 20, Training loss 1.451465807409368\n",
      "2022-03-27 20:58:13.370967 Epoch 30, Training loss 1.3455406036811677\n",
      "2022-03-27 20:59:45.430389 Epoch 40, Training loss 1.2813283663529615\n",
      "2022-03-27 21:01:18.358913 Epoch 50, Training loss 1.2366040116361743\n",
      "2022-03-27 21:02:50.134393 Epoch 60, Training loss 1.2026796055655193\n",
      "2022-03-27 21:04:22.033217 Epoch 70, Training loss 1.1744809794290114\n",
      "2022-03-27 21:05:53.329474 Epoch 80, Training loss 1.154808791614326\n",
      "2022-03-27 21:07:25.575955 Epoch 90, Training loss 1.1357732281046375\n",
      "2022-03-27 21:08:56.341949 Epoch 100, Training loss 1.118267748770211\n",
      "2022-03-27 21:10:27.897075 Epoch 110, Training loss 1.0911153993035994\n",
      "2022-03-27 21:11:59.766669 Epoch 120, Training loss 1.0807854803199441\n",
      "2022-03-27 21:13:31.493823 Epoch 130, Training loss 1.0669893011068687\n",
      "2022-03-27 21:15:03.623715 Epoch 140, Training loss 1.0564989909487232\n",
      "2022-03-27 21:16:35.701074 Epoch 150, Training loss 1.0481387490560525\n",
      "2022-03-27 21:18:07.143206 Epoch 160, Training loss 1.0416404767933054\n",
      "2022-03-27 21:19:38.494293 Epoch 170, Training loss 1.0300956587506156\n",
      "2022-03-27 21:21:09.174025 Epoch 180, Training loss 1.0306963648891176\n",
      "2022-03-27 21:22:40.833169 Epoch 190, Training loss 1.0225931047713994\n",
      "2022-03-27 21:24:12.288093 Epoch 200, Training loss 1.016093834003492\n",
      "2022-03-27 21:25:44.574168 Epoch 210, Training loss 1.0100098004368296\n",
      "2022-03-27 21:27:17.458435 Epoch 220, Training loss 1.013006313234313\n",
      "2022-03-27 21:28:50.786851 Epoch 230, Training loss 1.0061363762939757\n",
      "2022-03-27 21:30:23.200295 Epoch 240, Training loss 1.003528210172626\n",
      "2022-03-27 21:31:55.603774 Epoch 250, Training loss 0.9969140464424068\n",
      "2022-03-27 21:33:27.993451 Epoch 260, Training loss 1.000710596555998\n",
      "2022-03-27 21:35:00.129512 Epoch 270, Training loss 0.9947291546397738\n",
      "2022-03-27 21:36:31.916978 Epoch 280, Training loss 0.9929645205834652\n",
      "2022-03-27 21:38:04.389451 Epoch 290, Training loss 0.9911120817532227\n",
      "2022-03-27 21:39:36.363906 Epoch 300, Training loss 0.9889720965314794\n",
      "Accuracy train: 0.65\n",
      "Accuracy val: 0.64\n"
     ]
    }
   ],
   "source": [
    "model = NetB()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = NetB().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2) \n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "\n",
    "training_loop(  \n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c3ba134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION 2, PART A\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3, padding=1, bias=False)\n",
    "        #torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = torch.relu(out)\n",
    "        return out + x #Skip connection\n",
    "\n",
    "class ResNet10(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7786904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76010 [864, 32, 9216, 65536, 32, 320, 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6081e710ee5b4873a1e863d64fbfb805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 21:40:02.401241 Epoch 1, Training loss 1.9797430626007906\n",
      "2022-03-27 21:41:38.896724 Epoch 10, Training loss 1.1677336658847297\n",
      "2022-03-27 21:43:23.349446 Epoch 20, Training loss 0.9765184446957037\n",
      "2022-03-27 21:45:05.453244 Epoch 30, Training loss 0.8863915761991105\n",
      "2022-03-27 21:46:49.266637 Epoch 40, Training loss 0.81784396259873\n",
      "2022-03-27 21:48:34.407654 Epoch 50, Training loss 0.7695781147038495\n",
      "2022-03-27 21:50:17.148260 Epoch 60, Training loss 0.7313196742296898\n",
      "2022-03-27 21:52:00.740259 Epoch 70, Training loss 0.7026950527629962\n",
      "2022-03-27 21:53:44.930398 Epoch 80, Training loss 0.6767093193666888\n",
      "2022-03-27 21:55:29.906883 Epoch 90, Training loss 0.6593405956726128\n",
      "2022-03-27 21:57:13.548985 Epoch 100, Training loss 0.6365604016006502\n",
      "2022-03-27 21:58:55.465481 Epoch 110, Training loss 0.6165899555397849\n",
      "2022-03-27 22:00:37.769722 Epoch 120, Training loss 0.6061041385699542\n",
      "2022-03-27 22:02:19.843832 Epoch 130, Training loss 0.5881980646539617\n",
      "2022-03-27 22:04:01.740693 Epoch 140, Training loss 0.5822365539535838\n",
      "2022-03-27 22:05:48.801462 Epoch 150, Training loss 0.5680104697022343\n",
      "2022-03-27 22:07:37.400568 Epoch 160, Training loss 0.5578773415666021\n",
      "2022-03-27 22:09:23.879771 Epoch 170, Training loss 0.5536108652920465\n",
      "2022-03-27 22:11:04.506537 Epoch 180, Training loss 0.5400912275171688\n",
      "2022-03-27 22:12:46.841573 Epoch 190, Training loss 0.5361519963653\n",
      "2022-03-27 22:14:31.678584 Epoch 200, Training loss 0.5269890564289528\n",
      "2022-03-27 22:16:15.541226 Epoch 210, Training loss 0.520337383122186\n",
      "2022-03-27 22:17:56.649262 Epoch 220, Training loss 0.5181745160848666\n",
      "2022-03-27 22:19:37.137852 Epoch 230, Training loss 0.5092497035818562\n",
      "2022-03-27 22:21:18.018114 Epoch 240, Training loss 0.5021255153027016\n",
      "2022-03-27 22:22:59.132660 Epoch 250, Training loss 0.49563892172951984\n",
      "2022-03-27 22:24:40.911167 Epoch 260, Training loss 0.49193628637539355\n",
      "2022-03-27 22:26:22.950737 Epoch 270, Training loss 0.4900616898156299\n",
      "2022-03-27 22:28:06.323101 Epoch 280, Training loss 0.4840808228040353\n",
      "2022-03-27 22:29:52.517524 Epoch 290, Training loss 0.4795625845761041\n",
      "2022-03-27 22:31:37.620667 Epoch 300, Training loss 0.4753875380482769\n",
      "Accuracy train: 0.82\n",
      "Accuracy val: 0.75\n"
     ]
    }
   ],
   "source": [
    "model = ResNet10()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = ResNet10().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86da7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION 2, PART B\n",
    "    \n",
    "#Weight Decay training loop:\n",
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "    \n",
    "#Dropout versions:\n",
    "class ResBlock_DO(nn.Module):\n",
    "    def __init__(self, n_chans, p):\n",
    "        super(ResBlock_DO, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(p = p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.dropout(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "\n",
    "class ResNet10_DO(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10, p=0.3):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock_DO(n_chans=n_chans1, p=p)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "#Batch Normalization versions:\n",
    "class ResBlock_BN(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock_BN, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x\n",
    "\n",
    "class ResNet10_BN(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock_BN(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1413bd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76010 [864, 32, 9216, 65536, 32, 320, 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b19bdb23ed74b499b82f9b83d0242ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 22:32:04.045584 Epoch 1, Training loss 2.06732132631829\n",
      "2022-03-27 22:33:40.403937 Epoch 10, Training loss 1.1950943663588955\n",
      "2022-03-27 22:35:26.586354 Epoch 20, Training loss 1.0073482309311543\n",
      "2022-03-27 22:37:12.937124 Epoch 30, Training loss 0.9065220274476924\n",
      "2022-03-27 22:38:57.815567 Epoch 40, Training loss 0.8512656824541228\n",
      "2022-03-27 22:40:43.903413 Epoch 50, Training loss 0.7997998055229839\n",
      "2022-03-27 22:42:29.953127 Epoch 60, Training loss 0.764399053194584\n",
      "2022-03-27 22:44:16.623501 Epoch 70, Training loss 0.7323032103033147\n",
      "2022-03-27 22:46:02.721110 Epoch 80, Training loss 0.7134685900136616\n",
      "2022-03-27 22:47:48.535444 Epoch 90, Training loss 0.6885397566689385\n",
      "2022-03-27 22:49:34.095258 Epoch 100, Training loss 0.6665314722944189\n",
      "2022-03-27 22:51:19.315286 Epoch 110, Training loss 0.65073260469654\n",
      "2022-03-27 22:53:04.665270 Epoch 120, Training loss 0.6374655513681917\n",
      "2022-03-27 22:54:51.537534 Epoch 130, Training loss 0.6256482385502242\n",
      "2022-03-27 22:56:37.082574 Epoch 140, Training loss 0.6099192315017395\n",
      "2022-03-27 22:58:23.854947 Epoch 150, Training loss 0.598588037100273\n",
      "2022-03-27 23:00:14.592046 Epoch 160, Training loss 0.5881215085161378\n",
      "2022-03-27 23:02:05.906438 Epoch 170, Training loss 0.5869823987157936\n",
      "2022-03-27 23:03:54.896203 Epoch 180, Training loss 0.5743135985655662\n",
      "2022-03-27 23:05:41.032906 Epoch 190, Training loss 0.5672533239564325\n",
      "2022-03-27 23:07:28.614023 Epoch 200, Training loss 0.5624254326365272\n",
      "2022-03-27 23:09:16.169716 Epoch 210, Training loss 0.5582541619610583\n",
      "2022-03-27 23:11:02.668305 Epoch 220, Training loss 0.5503797773961667\n",
      "2022-03-27 23:12:52.498340 Epoch 230, Training loss 0.5448907574697098\n",
      "2022-03-27 23:14:38.930019 Epoch 240, Training loss 0.5427124459519346\n",
      "2022-03-27 23:16:27.857988 Epoch 250, Training loss 0.5391644112914377\n",
      "2022-03-27 23:18:17.928847 Epoch 260, Training loss 0.5335655889110348\n",
      "2022-03-27 23:20:04.358828 Epoch 270, Training loss 0.5279953667411098\n",
      "2022-03-27 23:21:49.823810 Epoch 280, Training loss 0.5232577139665598\n",
      "2022-03-27 23:23:35.034333 Epoch 290, Training loss 0.5256634766389842\n",
      "2022-03-27 23:25:20.271228 Epoch 300, Training loss 0.5199108164534609\n",
      "Accuracy train: 0.84\n",
      "Accuracy val: 0.76\n"
     ]
    }
   ],
   "source": [
    "#Weight Decay Model Results:\n",
    "model = ResNet10()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = ResNet10().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc3692e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76010 [864, 32, 9216, 65536, 32, 320, 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465504f68b134b559b061d72bcf82231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 23:25:45.999684 Epoch 1, Training loss 2.0476226314180597\n",
      "2022-03-27 23:27:21.213727 Epoch 10, Training loss 1.273833913341207\n",
      "2022-03-27 23:29:06.743294 Epoch 20, Training loss 1.0910009475175473\n",
      "2022-03-27 23:30:52.702578 Epoch 30, Training loss 0.9887161056200663\n",
      "2022-03-27 23:32:37.824351 Epoch 40, Training loss 0.9203331108786102\n",
      "2022-03-27 23:34:23.391019 Epoch 50, Training loss 0.8666891058286031\n",
      "2022-03-27 23:36:09.447284 Epoch 60, Training loss 0.8292987061361982\n",
      "2022-03-27 23:37:54.604488 Epoch 70, Training loss 0.79926557435609\n",
      "2022-03-27 23:39:39.923635 Epoch 80, Training loss 0.7705867932393\n",
      "2022-03-27 23:41:25.445445 Epoch 90, Training loss 0.7494173216344284\n",
      "2022-03-27 23:43:10.033194 Epoch 100, Training loss 0.7276362019386726\n",
      "2022-03-27 23:44:55.944332 Epoch 110, Training loss 0.7110369973033243\n",
      "2022-03-27 23:46:41.120827 Epoch 120, Training loss 0.6946421992405187\n",
      "2022-03-27 23:48:26.677799 Epoch 130, Training loss 0.6807130771314996\n",
      "2022-03-27 23:50:12.199207 Epoch 140, Training loss 0.6663021676221125\n",
      "2022-03-27 23:51:58.036670 Epoch 150, Training loss 0.6544608430475252\n",
      "2022-03-27 23:53:43.868018 Epoch 160, Training loss 0.6434519013278505\n",
      "2022-03-27 23:55:29.346263 Epoch 170, Training loss 0.6381661549762443\n",
      "2022-03-27 23:57:15.214278 Epoch 180, Training loss 0.6301488394071574\n",
      "2022-03-27 23:59:01.637678 Epoch 190, Training loss 0.6222615748049527\n",
      "2022-03-28 00:00:48.229750 Epoch 200, Training loss 0.6134154337254005\n",
      "2022-03-28 00:02:33.701381 Epoch 210, Training loss 0.6043865442106188\n",
      "2022-03-28 00:04:18.954937 Epoch 220, Training loss 0.5971222485911812\n",
      "2022-03-28 00:06:04.261729 Epoch 230, Training loss 0.5906492464902394\n",
      "2022-03-28 00:07:49.945443 Epoch 240, Training loss 0.5845749755190988\n",
      "2022-03-28 00:09:35.042085 Epoch 250, Training loss 0.5802840956425734\n",
      "2022-03-28 00:11:20.527846 Epoch 260, Training loss 0.5741157988537411\n",
      "2022-03-28 00:13:05.680697 Epoch 270, Training loss 0.5712010386662606\n",
      "2022-03-28 00:14:50.499028 Epoch 280, Training loss 0.5634800534472506\n",
      "2022-03-28 00:16:35.121751 Epoch 290, Training loss 0.564233284475457\n",
      "2022-03-28 00:18:19.976995 Epoch 300, Training loss 0.5569530486041664\n",
      "Accuracy train: 0.80\n",
      "Accuracy val: 0.76\n"
     ]
    }
   ],
   "source": [
    "#Dropout Model Results:\n",
    "model = ResNet10_DO()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = ResNet10_DO().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fa9e768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76074 [864, 32, 9216, 32, 32, 65536, 32, 320, 10]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628c5b16db194dfcb34e6284f1658ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-28 00:18:47.003608 Epoch 1, Training loss 1.7722551466053367\n",
      "2022-03-28 00:20:23.385047 Epoch 10, Training loss 1.152346660268952\n",
      "2022-03-28 00:22:10.889077 Epoch 20, Training loss 1.0021720083690437\n",
      "2022-03-28 00:23:58.638209 Epoch 30, Training loss 0.913266035396489\n",
      "2022-03-28 00:25:46.022947 Epoch 40, Training loss 0.8590302934334149\n",
      "2022-03-28 00:27:34.365174 Epoch 50, Training loss 0.8096600222451734\n",
      "2022-03-28 00:29:22.343516 Epoch 60, Training loss 0.7688918594281557\n",
      "2022-03-28 00:31:10.340008 Epoch 70, Training loss 0.7381861104584827\n",
      "2022-03-28 00:32:59.179251 Epoch 80, Training loss 0.7125981613102123\n",
      "2022-03-28 00:34:47.713639 Epoch 90, Training loss 0.6877301449110026\n",
      "2022-03-28 00:36:36.442226 Epoch 100, Training loss 0.6673962031504368\n",
      "2022-03-28 00:38:25.406701 Epoch 110, Training loss 0.6520670902864886\n",
      "2022-03-28 00:40:13.321896 Epoch 120, Training loss 0.6382100237400783\n",
      "2022-03-28 00:42:01.582939 Epoch 130, Training loss 0.6166405405244256\n",
      "2022-03-28 00:43:50.182175 Epoch 140, Training loss 0.604415263715293\n",
      "2022-03-28 00:45:37.983296 Epoch 150, Training loss 0.5946431808661871\n",
      "2022-03-28 00:47:26.442999 Epoch 160, Training loss 0.5812264935583131\n",
      "2022-03-28 00:49:14.869264 Epoch 170, Training loss 0.5671288398256329\n",
      "2022-03-28 00:51:02.754212 Epoch 180, Training loss 0.5603279769420624\n",
      "2022-03-28 00:52:50.731411 Epoch 190, Training loss 0.5522240361257157\n",
      "2022-03-28 00:54:38.573322 Epoch 200, Training loss 0.5409876764499904\n",
      "2022-03-28 00:56:26.653307 Epoch 210, Training loss 0.5401057048231108\n",
      "2022-03-28 00:58:14.526218 Epoch 220, Training loss 0.5313336735437398\n",
      "2022-03-28 01:00:02.756343 Epoch 230, Training loss 0.520323050412697\n",
      "2022-03-28 01:01:50.873376 Epoch 240, Training loss 0.5130281585064369\n",
      "2022-03-28 01:03:38.842572 Epoch 250, Training loss 0.5107846056294237\n",
      "2022-03-28 01:05:27.435546 Epoch 260, Training loss 0.5077723985553807\n",
      "2022-03-28 01:07:15.592937 Epoch 270, Training loss 0.4967092571095524\n",
      "2022-03-28 01:09:03.718063 Epoch 280, Training loss 0.49469763804704714\n",
      "2022-03-28 01:10:51.657066 Epoch 290, Training loss 0.49025193408683493\n",
      "2022-03-28 01:12:39.847049 Epoch 300, Training loss 0.4860874059533122\n",
      "Accuracy train: 0.83\n",
      "Accuracy val: 0.77\n"
     ]
    }
   ],
   "source": [
    "#Batch Norm Model Results:\n",
    "model = ResNet10_BN()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "model = ResNet10_BN().to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de644efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
