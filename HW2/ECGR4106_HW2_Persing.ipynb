{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5bce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569d1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "#Define functions\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, in_train, in_val, out_train, out_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        p_train = model(in_train) # <1>\n",
    "        loss_train = loss_fn(p_train, out_train)\n",
    "\n",
    "        p_val = model(in_val) # <1>\n",
    "        loss_val = loss_fn(p_val, out_val)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward() # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(datetime.datetime.now(), f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "                  f\" Validation loss {loss_val.item():.4f}\")\n",
    "            \n",
    "def binary_map(x):\n",
    "    return x.map({'yes':1,\"no\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc3d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "housing = pd.DataFrame(pd.read_csv(\"./Housing.csv\")) \n",
    "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'price']\n",
    "varList=['mainroad','guestroom','basement','hotwaterheating','airconditioning', 'prefarea']\n",
    "input_size = len(num_vars)-1\n",
    "\n",
    "housing[varList] = housing[varList].apply(binary_map)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "#Split data into training and validation sets\n",
    "df_train, df_test = train_test_split(housing, train_size=0.8, test_size=0.2, random_state=seed)\n",
    "\n",
    "df_Newtrain = df_train[num_vars]\n",
    "df_Newtest = df_test[num_vars]\n",
    "\n",
    "# scaling the data\n",
    "df_Newtrain[num_vars] = scaler.fit_transform(df_Newtrain[num_vars])\n",
    "df_Newtest[num_vars] = scaler.fit_transform(df_Newtest[num_vars])\n",
    "\n",
    "#Create input and output arrays for both training and validation\n",
    "out_Newtrain = df_Newtrain.pop('price')\n",
    "in_Newtrain = df_Newtrain\n",
    "out_Newtest = df_Newtest.pop('price')\n",
    "in_Newtest = df_Newtest\n",
    "\n",
    "# convert the data to tensors\n",
    "in_train = torch.tensor(in_Newtrain.values).float()\n",
    "in_val = torch.tensor(in_Newtest.values).float()\n",
    "out_train = torch.tensor(out_Newtrain.values).float().unsqueeze(-1)\n",
    "out_val = torch.tensor(out_Newtest.values).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68acd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-03 14:52:15.892236 Epoch 1, Training loss 0.9592, Validation loss 0.9232\n",
      "2022-03-03 14:52:15.897223 Epoch 10, Training loss 0.9464, Validation loss 0.9090\n",
      "2022-03-03 14:52:15.901212 Epoch 20, Training loss 0.9326, Validation loss 0.8937\n",
      "2022-03-03 14:52:15.908194 Epoch 30, Training loss 0.9192, Validation loss 0.8789\n",
      "2022-03-03 14:52:15.913180 Epoch 40, Training loss 0.9063, Validation loss 0.8645\n",
      "2022-03-03 14:52:15.917169 Epoch 50, Training loss 0.8937, Validation loss 0.8505\n",
      "2022-03-03 14:52:15.922156 Epoch 60, Training loss 0.8814, Validation loss 0.8369\n",
      "2022-03-03 14:52:15.927143 Epoch 70, Training loss 0.8696, Validation loss 0.8237\n",
      "2022-03-03 14:52:15.932129 Epoch 80, Training loss 0.8580, Validation loss 0.8109\n",
      "2022-03-03 14:52:15.937118 Epoch 90, Training loss 0.8468, Validation loss 0.7984\n",
      "2022-03-03 14:52:15.942104 Epoch 100, Training loss 0.8358, Validation loss 0.7863\n",
      "2022-03-03 14:52:15.947091 Epoch 110, Training loss 0.8252, Validation loss 0.7745\n",
      "2022-03-03 14:52:15.952076 Epoch 120, Training loss 0.8148, Validation loss 0.7630\n",
      "2022-03-03 14:52:15.956065 Epoch 130, Training loss 0.8047, Validation loss 0.7518\n",
      "2022-03-03 14:52:15.961052 Epoch 140, Training loss 0.7948, Validation loss 0.7409\n",
      "2022-03-03 14:52:15.965041 Epoch 150, Training loss 0.7852, Validation loss 0.7303\n",
      "2022-03-03 14:52:15.970059 Epoch 160, Training loss 0.7758, Validation loss 0.7200\n",
      "2022-03-03 14:52:15.975049 Epoch 170, Training loss 0.7667, Validation loss 0.7099\n",
      "2022-03-03 14:52:15.980034 Epoch 180, Training loss 0.7577, Validation loss 0.7001\n",
      "2022-03-03 14:52:15.984023 Epoch 190, Training loss 0.7490, Validation loss 0.6905\n",
      "2022-03-03 14:52:15.989013 Epoch 200, Training loss 0.7405, Validation loss 0.6812\n"
     ]
    }
   ],
   "source": [
    "#Question 1, part 1\n",
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(input_size, 8), #hidden layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 1)) #output layer\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 200, \n",
    "    optimizer = optimizer,\n",
    "    model = seq_model,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    in_train = in_train,\n",
    "    in_val = in_val, \n",
    "    out_train = out_train,\n",
    "    out_val = out_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128b9954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-03 14:52:16.003946 Epoch 1, Training loss 1.0192, Validation loss 1.0279\n",
      "2022-03-03 14:52:16.012916 Epoch 10, Training loss 1.0037, Validation loss 1.0110\n",
      "2022-03-03 14:52:16.019934 Epoch 20, Training loss 0.9873, Validation loss 0.9931\n",
      "2022-03-03 14:52:16.026877 Epoch 30, Training loss 0.9717, Validation loss 0.9759\n",
      "2022-03-03 14:52:16.033859 Epoch 40, Training loss 0.9568, Validation loss 0.9594\n",
      "2022-03-03 14:52:16.039843 Epoch 50, Training loss 0.9425, Validation loss 0.9437\n",
      "2022-03-03 14:52:16.046858 Epoch 60, Training loss 0.9288, Validation loss 0.9285\n",
      "2022-03-03 14:52:16.052808 Epoch 70, Training loss 0.9157, Validation loss 0.9140\n",
      "2022-03-03 14:52:16.057826 Epoch 80, Training loss 0.9031, Validation loss 0.9000\n",
      "2022-03-03 14:52:16.063778 Epoch 90, Training loss 0.8910, Validation loss 0.8865\n",
      "2022-03-03 14:52:16.069761 Epoch 100, Training loss 0.8793, Validation loss 0.8734\n",
      "2022-03-03 14:52:16.075746 Epoch 110, Training loss 0.8681, Validation loss 0.8609\n",
      "2022-03-03 14:52:16.084758 Epoch 120, Training loss 0.8573, Validation loss 0.8487\n",
      "2022-03-03 14:52:16.094731 Epoch 130, Training loss 0.8468, Validation loss 0.8370\n",
      "2022-03-03 14:52:16.100715 Epoch 140, Training loss 0.8366, Validation loss 0.8256\n",
      "2022-03-03 14:52:16.106697 Epoch 150, Training loss 0.8268, Validation loss 0.8145\n",
      "2022-03-03 14:52:16.112681 Epoch 160, Training loss 0.8173, Validation loss 0.8038\n",
      "2022-03-03 14:52:16.118665 Epoch 170, Training loss 0.8081, Validation loss 0.7934\n",
      "2022-03-03 14:52:16.124651 Epoch 180, Training loss 0.7991, Validation loss 0.7833\n",
      "2022-03-03 14:52:16.130638 Epoch 190, Training loss 0.7904, Validation loss 0.7735\n",
      "2022-03-03 14:52:16.136616 Epoch 200, Training loss 0.7819, Validation loss 0.7639\n"
     ]
    }
   ],
   "source": [
    "#Question 1, part 2\n",
    "seq_model_2 = nn.Sequential(\n",
    "            nn.Linear(input_size, 8), #hidden layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 4), #hidden layer 2\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 1)) #output layer\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(seq_model_2.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 200, \n",
    "    optimizer = optimizer,\n",
    "    model = seq_model_2,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    in_train = in_train,\n",
    "    in_val = in_val, \n",
    "    out_train = out_train,\n",
    "    out_val = out_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10282603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Question 2:\n",
    "\n",
    "#Load and preprocess dataset\n",
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_train = [(img, label) for img, label in cifar10]\n",
    "cifar10_test = [(img, label) for img, label in cifar10_val]\n",
    "\n",
    "#Use GPU if available\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7def60fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-03 14:52:33.691791 Epoch 0, Training loss 1.8294525146484375\n",
      "2022-03-03 14:52:45.007460 Epoch 10, Training loss 1.5903112888336182\n",
      "2022-03-03 14:52:56.404122 Epoch 20, Training loss 0.8837257027626038\n",
      "2022-03-03 14:53:07.802017 Epoch 30, Training loss 0.8088313937187195\n",
      "2022-03-03 14:53:19.669580 Epoch 40, Training loss 0.31869271397590637\n",
      "2022-03-03 14:53:33.029318 Epoch 50, Training loss 0.22469337284564972\n",
      "2022-03-03 14:53:44.976459 Epoch 60, Training loss 0.3721758723258972\n",
      "2022-03-03 14:53:56.718847 Epoch 70, Training loss 0.2668774127960205\n",
      "2022-03-03 14:54:08.434106 Epoch 80, Training loss 0.08345070481300354\n",
      "2022-03-03 14:54:20.243292 Epoch 90, Training loss 0.04918540269136429\n",
      "2022-03-03 14:54:31.984102 Epoch 100, Training loss 0.03226412460207939\n",
      "2022-03-03 14:54:43.551045 Epoch 110, Training loss 0.015502438880503178\n",
      "2022-03-03 14:54:55.152285 Epoch 120, Training loss 0.019649691879749298\n",
      "2022-03-03 14:55:06.784636 Epoch 130, Training loss 0.025698699057102203\n",
      "2022-03-03 14:55:18.284003 Epoch 140, Training loss 0.02127782441675663\n",
      "2022-03-03 14:55:30.053612 Epoch 150, Training loss 0.025470130145549774\n",
      "2022-03-03 14:55:41.952663 Epoch 160, Training loss 0.018833940848708153\n",
      "2022-03-03 14:55:53.948976 Epoch 170, Training loss 0.01396147720515728\n",
      "2022-03-03 14:56:05.756888 Epoch 180, Training loss 0.011456046253442764\n",
      "2022-03-03 14:56:17.544118 Epoch 190, Training loss 0.01156251784414053\n",
      "2022-03-03 14:56:29.216449 Epoch 200, Training loss 0.010197975672781467\n",
      "2022-03-03 14:56:40.834919 Epoch 210, Training loss 0.009900171309709549\n",
      "2022-03-03 14:56:52.395748 Epoch 220, Training loss 0.007924389094114304\n",
      "2022-03-03 14:57:04.060011 Epoch 230, Training loss 0.00913307536393404\n",
      "2022-03-03 14:57:15.779515 Epoch 240, Training loss 0.01105607021600008\n",
      "2022-03-03 14:57:27.470060 Epoch 250, Training loss 0.008753187954425812\n",
      "2022-03-03 14:57:39.132953 Epoch 260, Training loss 0.00948468130081892\n",
      "2022-03-03 14:57:51.657768 Epoch 270, Training loss 0.007930590771138668\n",
      "2022-03-03 14:58:04.115499 Epoch 280, Training loss 0.006824629381299019\n",
      "2022-03-03 14:58:15.813617 Epoch 290, Training loss 0.006650552619248629\n",
      "2022-03-03 14:58:27.607994 Epoch 300, Training loss 0.006457412615418434\n",
      "Train Accuracy: 1.000000\n",
      "Val Accuracy: 0.463900\n"
     ]
    }
   ],
   "source": [
    "#Question 2, Part 1:\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=True)\n",
    "\n",
    "model_cifar = nn.Sequential(\n",
    "            nn.Linear(3072, 512), #Hidden Layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 10)) #Output layer\n",
    "\n",
    "model_cifar.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model_cifar.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 301\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "      print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), epoch, loss)) \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Train Accuracy: %f\" % (correct / total))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Val Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50844c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-03 14:58:30.020766 Epoch 0, Training loss 1.7073450088500977\n",
      "2022-03-03 14:58:45.645252 Epoch 10, Training loss 1.473246455192566\n",
      "2022-03-03 14:59:00.630556 Epoch 20, Training loss 1.0912140607833862\n",
      "2022-03-03 14:59:15.596679 Epoch 30, Training loss 0.5794255137443542\n",
      "2022-03-03 14:59:30.451327 Epoch 40, Training loss 0.28757891058921814\n",
      "2022-03-03 14:59:46.059999 Epoch 50, Training loss 0.05835846811532974\n",
      "2022-03-03 15:00:01.419002 Epoch 60, Training loss 0.02068966068327427\n",
      "2022-03-03 15:00:18.586125 Epoch 70, Training loss 0.01195518858730793\n",
      "2022-03-03 15:00:34.404188 Epoch 80, Training loss 0.004421709105372429\n",
      "2022-03-03 15:00:49.796586 Epoch 90, Training loss 0.006905160844326019\n",
      "2022-03-03 15:01:04.876555 Epoch 100, Training loss 0.0030110320076346397\n",
      "2022-03-03 15:01:19.957236 Epoch 110, Training loss 0.0021306113339960575\n",
      "2022-03-03 15:01:35.237171 Epoch 120, Training loss 0.0025274697691202164\n",
      "2022-03-03 15:01:50.437061 Epoch 130, Training loss 0.0021563915070146322\n",
      "2022-03-03 15:02:05.779713 Epoch 140, Training loss 0.0014866618439555168\n",
      "2022-03-03 15:02:21.071975 Epoch 150, Training loss 0.0021987655200064182\n",
      "2022-03-03 15:02:36.324080 Epoch 160, Training loss 0.0009377512033097446\n",
      "2022-03-03 15:02:51.470476 Epoch 170, Training loss 0.0009960262104868889\n",
      "2022-03-03 15:03:06.550851 Epoch 180, Training loss 0.0011967822210863233\n",
      "2022-03-03 15:03:21.474149 Epoch 190, Training loss 0.0009037317940965295\n",
      "2022-03-03 15:03:36.258670 Epoch 200, Training loss 0.0008028874290175736\n",
      "2022-03-03 15:03:51.083734 Epoch 210, Training loss 0.0008436356438323855\n",
      "2022-03-03 15:04:05.906596 Epoch 220, Training loss 0.0008469566237181425\n",
      "2022-03-03 15:04:20.795793 Epoch 230, Training loss 0.0006409930065274239\n",
      "2022-03-03 15:04:35.560693 Epoch 240, Training loss 0.0006820749258622527\n",
      "2022-03-03 15:04:50.363361 Epoch 250, Training loss 0.0005731938872486353\n",
      "2022-03-03 15:05:05.209963 Epoch 260, Training loss 0.0009556603617966175\n",
      "2022-03-03 15:05:20.010361 Epoch 270, Training loss 0.0005443613044917583\n",
      "2022-03-03 15:05:34.804437 Epoch 280, Training loss 0.00043910916429013014\n",
      "2022-03-03 15:05:49.587951 Epoch 290, Training loss 0.0008800585055723786\n",
      "2022-03-03 15:06:04.371149 Epoch 300, Training loss 0.0006281242240220308\n",
      "Train Accuracy: 1.000000\n",
      "Val Accuracy: 0.456400\n"
     ]
    }
   ],
   "source": [
    "#Question 2, Part 1:\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=True)\n",
    "\n",
    "model_cifar_2 = nn.Sequential(\n",
    "            nn.Linear(3072, 512), #hidden layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1024), #hidden layer 2\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 10)) #output layer\n",
    "\n",
    "model_cifar_2.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model_cifar_2.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 301\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "      print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), epoch, loss))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Train Accuracy: %f\" % (correct / total))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Val Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557b508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ce2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
