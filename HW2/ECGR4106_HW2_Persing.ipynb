{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5bce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569d1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "#Define functions\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, in_train, in_val, out_train, out_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        p_train = model(in_train) # <1>\n",
    "        loss_train = loss_fn(p_train, out_train)\n",
    "\n",
    "        p_val = model(in_val) # <1>\n",
    "        loss_val = loss_fn(p_val, out_val)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward() # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(datetime.datetime.now(), f\"Epoch {epoch}, Training loss {loss_train.item():.4f},\"\n",
    "                  f\" Validation loss {loss_val.item():.4f}\")\n",
    "            \n",
    "def binary_map(x):\n",
    "    return x.map({'yes':1,\"no\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc3d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "housing = pd.DataFrame(pd.read_csv(\"./Housing.csv\")) \n",
    "num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'parking', 'prefarea', 'price']\n",
    "varList=['mainroad','guestroom','basement','hotwaterheating','airconditioning', 'prefarea']\n",
    "input_size = len(num_vars)-1\n",
    "\n",
    "housing[varList] = housing[varList].apply(binary_map)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "#Split data into training and validation sets\n",
    "df_train, df_test = train_test_split(housing, train_size=0.8, test_size=0.2, random_state=seed)\n",
    "\n",
    "df_Newtrain = df_train[num_vars]\n",
    "df_Newtest = df_test[num_vars]\n",
    "\n",
    "# scaling the data\n",
    "df_Newtrain[num_vars] = scaler.fit_transform(df_Newtrain[num_vars])\n",
    "df_Newtest[num_vars] = scaler.fit_transform(df_Newtest[num_vars])\n",
    "\n",
    "#Create input and output arrays for both training and validation\n",
    "out_Newtrain = df_Newtrain.pop('price')\n",
    "in_Newtrain = df_Newtrain\n",
    "out_Newtest = df_Newtest.pop('price')\n",
    "in_Newtest = df_Newtest\n",
    "\n",
    "# convert the data to tensors\n",
    "in_train = torch.tensor(in_Newtrain.values).float()\n",
    "in_val = torch.tensor(in_Newtest.values).float()\n",
    "out_train = torch.tensor(out_Newtrain.values).float().unsqueeze(-1)\n",
    "out_val = torch.tensor(out_Newtest.values).float().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e68acd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-04 19:31:55.414809 Epoch 1, Training loss 1.0984, Validation loss 1.0486\n",
      "2022-03-04 19:31:55.418088 Epoch 10, Training loss 1.0850, Validation loss 1.0331\n",
      "2022-03-04 19:31:55.428843 Epoch 20, Training loss 1.0706, Validation loss 1.0164\n",
      "2022-03-04 19:31:55.440536 Epoch 30, Training loss 1.0567, Validation loss 1.0003\n",
      "2022-03-04 19:31:55.452157 Epoch 40, Training loss 1.0433, Validation loss 0.9847\n",
      "2022-03-04 19:31:55.463712 Epoch 50, Training loss 1.0303, Validation loss 0.9697\n",
      "2022-03-04 19:31:55.473631 Epoch 60, Training loss 1.0178, Validation loss 0.9551\n",
      "2022-03-04 19:31:55.484770 Epoch 70, Training loss 1.0056, Validation loss 0.9409\n",
      "2022-03-04 19:31:55.489877 Epoch 80, Training loss 0.9939, Validation loss 0.9271\n",
      "2022-03-04 19:31:55.499696 Epoch 90, Training loss 0.9824, Validation loss 0.9138\n",
      "2022-03-04 19:31:55.507637 Epoch 100, Training loss 0.9713, Validation loss 0.9008\n",
      "2022-03-04 19:31:55.527026 Epoch 110, Training loss 0.9606, Validation loss 0.8882\n",
      "2022-03-04 19:31:55.529021 Epoch 120, Training loss 0.9501, Validation loss 0.8759\n",
      "2022-03-04 19:31:55.539377 Epoch 130, Training loss 0.9398, Validation loss 0.8639\n",
      "2022-03-04 19:31:55.549418 Epoch 140, Training loss 0.9299, Validation loss 0.8522\n",
      "2022-03-04 19:31:55.554375 Epoch 150, Training loss 0.9202, Validation loss 0.8408\n",
      "2022-03-04 19:31:55.560936 Epoch 160, Training loss 0.9107, Validation loss 0.8297\n",
      "2022-03-04 19:31:55.568247 Epoch 170, Training loss 0.9015, Validation loss 0.8188\n",
      "2022-03-04 19:31:55.574732 Epoch 180, Training loss 0.8924, Validation loss 0.8082\n",
      "2022-03-04 19:31:55.579517 Epoch 190, Training loss 0.8836, Validation loss 0.7978\n",
      "2022-03-04 19:31:55.584963 Epoch 200, Training loss 0.8750, Validation loss 0.7876\n"
     ]
    }
   ],
   "source": [
    "#Question 1, part 1\n",
    "seq_model = nn.Sequential(\n",
    "            nn.Linear(input_size, 8), #hidden layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 1)) #output layer\n",
    "\n",
    "optimizer = optim.SGD(seq_model.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 200, \n",
    "    optimizer = optimizer,\n",
    "    model = seq_model,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    in_train = in_train,\n",
    "    in_val = in_val, \n",
    "    out_train = out_train,\n",
    "    out_val = out_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "128b9954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-04 19:31:55.618988 Epoch 1, Training loss 1.4355, Validation loss 1.4379\n",
      "2022-03-04 19:31:55.628790 Epoch 10, Training loss 1.4025, Validation loss 1.4044\n",
      "2022-03-04 19:31:55.639816 Epoch 20, Training loss 1.3683, Validation loss 1.3695\n",
      "2022-03-04 19:31:55.649982 Epoch 30, Training loss 1.3364, Validation loss 1.3369\n",
      "2022-03-04 19:31:55.658346 Epoch 40, Training loss 1.3065, Validation loss 1.3065\n",
      "2022-03-04 19:31:55.668024 Epoch 50, Training loss 1.2786, Validation loss 1.2780\n",
      "2022-03-04 19:31:55.670087 Epoch 60, Training loss 1.2525, Validation loss 1.2513\n",
      "2022-03-04 19:31:55.690871 Epoch 70, Training loss 1.2280, Validation loss 1.2263\n",
      "2022-03-04 19:31:55.692748 Epoch 80, Training loss 1.2050, Validation loss 1.2028\n",
      "2022-03-04 19:31:55.701586 Epoch 90, Training loss 1.1833, Validation loss 1.1806\n",
      "2022-03-04 19:31:55.709986 Epoch 100, Training loss 1.1629, Validation loss 1.1598\n",
      "2022-03-04 19:31:55.719288 Epoch 110, Training loss 1.1437, Validation loss 1.1401\n",
      "2022-03-04 19:31:55.726636 Epoch 120, Training loss 1.1256, Validation loss 1.1215\n",
      "2022-03-04 19:31:55.734983 Epoch 130, Training loss 1.1084, Validation loss 1.1039\n",
      "2022-03-04 19:31:55.745460 Epoch 140, Training loss 1.0922, Validation loss 1.0873\n",
      "2022-03-04 19:31:55.766365 Epoch 150, Training loss 1.0768, Validation loss 1.0715\n",
      "2022-03-04 19:31:55.767938 Epoch 160, Training loss 1.0622, Validation loss 1.0566\n",
      "2022-03-04 19:31:55.776305 Epoch 170, Training loss 1.0484, Validation loss 1.0423\n",
      "2022-03-04 19:31:55.786921 Epoch 180, Training loss 1.0352, Validation loss 1.0288\n",
      "2022-03-04 19:31:55.793227 Epoch 190, Training loss 1.0226, Validation loss 1.0159\n",
      "2022-03-04 19:31:55.802496 Epoch 200, Training loss 1.0107, Validation loss 1.0036\n"
     ]
    }
   ],
   "source": [
    "#Question 1, part 2\n",
    "seq_model_2 = nn.Sequential(\n",
    "            nn.Linear(input_size, 8), #hidden layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8, 4), #hidden layer 2\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(4, 2), #hidden layer 3\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2, 1)) #output layer\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(seq_model_2.parameters(), lr=1e-3)\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 200, \n",
    "    optimizer = optimizer,\n",
    "    model = seq_model_2,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    in_train = in_train,\n",
    "    in_val = in_val, \n",
    "    out_train = out_train,\n",
    "    out_val = out_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10282603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Question 2:\n",
    "\n",
    "#Load and preprocess dataset\n",
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_train = [(img, label) for img, label in cifar10]\n",
    "cifar10_test = [(img, label) for img, label in cifar10_val]\n",
    "\n",
    "#Use GPU if available\n",
    "device = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7def60fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-04 19:32:20.922285 Epoch 0, Training loss 2.089669942855835\n",
      "2022-03-04 19:32:35.645967 Epoch 10, Training loss 1.3846532106399536\n",
      "2022-03-04 19:32:50.611277 Epoch 20, Training loss 1.3014476299285889\n",
      "2022-03-04 19:33:05.442218 Epoch 30, Training loss 0.8343853950500488\n",
      "2022-03-04 19:33:20.414386 Epoch 40, Training loss 0.2873496115207672\n",
      "2022-03-04 19:33:35.621919 Epoch 50, Training loss 0.5012293457984924\n",
      "2022-03-04 19:33:50.506790 Epoch 60, Training loss 0.18483269214630127\n",
      "2022-03-04 19:34:05.415690 Epoch 70, Training loss 0.14726722240447998\n",
      "2022-03-04 19:34:20.309935 Epoch 80, Training loss 0.0986277312040329\n",
      "2022-03-04 19:34:35.028396 Epoch 90, Training loss 0.05771343410015106\n",
      "2022-03-04 19:34:49.656886 Epoch 100, Training loss 0.04853050038218498\n",
      "2022-03-04 19:35:04.702568 Epoch 110, Training loss 0.02474435791373253\n",
      "2022-03-04 19:35:19.528246 Epoch 120, Training loss 0.01835634745657444\n",
      "2022-03-04 19:35:34.231601 Epoch 130, Training loss 0.027338389307260513\n",
      "2022-03-04 19:35:48.879477 Epoch 140, Training loss 0.013985136523842812\n",
      "2022-03-04 19:36:04.023693 Epoch 150, Training loss 0.02157716080546379\n",
      "2022-03-04 19:36:18.865828 Epoch 160, Training loss 0.014218855649232864\n",
      "2022-03-04 19:36:33.798337 Epoch 170, Training loss 0.015048476867377758\n",
      "2022-03-04 19:36:48.785838 Epoch 180, Training loss 0.008763990364968777\n",
      "2022-03-04 19:37:04.307448 Epoch 190, Training loss 0.005821674130856991\n",
      "2022-03-04 19:37:19.454572 Epoch 200, Training loss 0.02135135978460312\n",
      "2022-03-04 19:37:34.395594 Epoch 210, Training loss 0.010527201928198338\n",
      "2022-03-04 19:37:49.382583 Epoch 220, Training loss 0.005463793408125639\n",
      "2022-03-04 19:38:04.408640 Epoch 230, Training loss 0.010430803522467613\n",
      "2022-03-04 19:38:19.333108 Epoch 240, Training loss 0.010200269520282745\n",
      "2022-03-04 19:38:34.265811 Epoch 250, Training loss 0.007283372338861227\n",
      "2022-03-04 19:38:49.178115 Epoch 260, Training loss 0.007451111916452646\n",
      "2022-03-04 19:39:08.790058 Epoch 270, Training loss 0.007140572182834148\n",
      "2022-03-04 19:39:23.789416 Epoch 280, Training loss 0.004685676656663418\n",
      "2022-03-04 19:39:38.659567 Epoch 290, Training loss 0.00457304110750556\n",
      "2022-03-04 19:39:53.588926 Epoch 300, Training loss 0.005816466175019741\n",
      "Train Accuracy: 1.000000\n",
      "Val Accuracy: 0.463900\n"
     ]
    }
   ],
   "source": [
    "#Question 2, Part 1:\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=True)\n",
    "\n",
    "model_cifar = nn.Sequential(\n",
    "            nn.Linear(3072, 512), #Hidden Layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 10)) #Output layer\n",
    "\n",
    "model_cifar.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model_cifar.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 301\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "      print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), epoch, loss)) \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Train Accuracy: %f\" % (correct / total))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Val Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50844c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-04 19:39:57.107777 Epoch 0, Training loss 1.5523165464401245\n",
      "2022-03-04 19:40:23.170262 Epoch 10, Training loss 1.2748993635177612\n",
      "2022-03-04 19:40:51.795532 Epoch 20, Training loss 1.4034696817398071\n",
      "2022-03-04 19:41:14.767318 Epoch 30, Training loss 0.42979735136032104\n",
      "2022-03-04 19:41:37.978402 Epoch 40, Training loss 0.3505880832672119\n",
      "2022-03-04 19:42:02.356666 Epoch 50, Training loss 0.01225757785141468\n",
      "2022-03-04 19:42:34.338985 Epoch 60, Training loss 0.004762476775795221\n",
      "2022-03-04 19:42:57.891761 Epoch 70, Training loss 0.004877153318375349\n",
      "2022-03-04 19:43:20.841858 Epoch 80, Training loss 0.0021790186874568462\n",
      "2022-03-04 19:43:43.703571 Epoch 90, Training loss 0.0009847992332652211\n",
      "2022-03-04 19:44:06.697824 Epoch 100, Training loss 0.000773354375269264\n",
      "2022-03-04 19:44:33.467547 Epoch 110, Training loss 0.0007180912070907652\n",
      "2022-03-04 19:44:57.090393 Epoch 120, Training loss 0.00029932305915281177\n",
      "2022-03-04 19:45:23.988738 Epoch 130, Training loss 0.0007916414178907871\n",
      "2022-03-04 19:45:46.792517 Epoch 140, Training loss 0.00028470956021919847\n",
      "2022-03-04 19:46:13.938286 Epoch 150, Training loss 0.0004707238113041967\n",
      "2022-03-04 19:46:36.724812 Epoch 160, Training loss 0.0005113236256875098\n",
      "2022-03-04 19:46:59.917592 Epoch 170, Training loss 0.00022983024246059358\n",
      "2022-03-04 19:47:22.852496 Epoch 180, Training loss 0.00031695194775238633\n",
      "2022-03-04 19:47:53.451949 Epoch 190, Training loss 0.00042159599252045155\n",
      "2022-03-04 19:48:16.294255 Epoch 200, Training loss 0.0002673878916539252\n",
      "2022-03-04 19:48:39.126915 Epoch 210, Training loss 0.00042831205064430833\n",
      "2022-03-04 19:49:01.884174 Epoch 220, Training loss 0.0004106000706087798\n",
      "2022-03-04 19:49:24.521873 Epoch 230, Training loss 0.00032937622745521367\n",
      "2022-03-04 19:49:47.138140 Epoch 240, Training loss 0.000288467388600111\n",
      "2022-03-04 19:50:14.375908 Epoch 250, Training loss 0.0002892570337280631\n",
      "2022-03-04 19:50:37.263123 Epoch 260, Training loss 0.0003560274781193584\n",
      "2022-03-04 19:51:00.194903 Epoch 270, Training loss 0.00024441606365144253\n",
      "2022-03-04 19:51:23.147108 Epoch 280, Training loss 0.0002887442533392459\n",
      "2022-03-04 19:51:46.116114 Epoch 290, Training loss 0.00034633654286153615\n",
      "2022-03-04 19:52:09.369140 Epoch 300, Training loss 0.0003276073548477143\n",
      "Train Accuracy: 1.000000\n",
      "Val Accuracy: 0.455800\n"
     ]
    }
   ],
   "source": [
    "#Question 2, Part 1:\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=True)\n",
    "\n",
    "model_cifar_2 = nn.Sequential(\n",
    "            nn.Linear(3072, 512), #hidden layer 1\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1024), #hidden layer 2\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512), #hidden layer 3\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 10)) #output layer\n",
    "\n",
    "model_cifar_2.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model_cifar_2.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 301\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "      print('{} Epoch {}, Training loss {}'.format(datetime.datetime.now(), epoch, loss))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Train Accuracy: %f\" % (correct / total))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        outputs = model_cifar_2(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Val Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557b508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ce2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
